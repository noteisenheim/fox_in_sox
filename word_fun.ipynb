{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"today is your day\",\n",
    "    \"you are off to great places\",\n",
    "    \"you are off and away\",\n",
    "    \"you have brains in your head\",\n",
    "    \"you have feet in your shoes\",\n",
    "    \"you can steer yourself any direction you choose\",\n",
    "    \"you are on your own\",\n",
    "    \"and you know what you know\",\n",
    "    \"and you are the guy who will decide where to go\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating the learning base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#breaks the corpus into one-word tokens\n",
    "\n",
    "def tokenize(corpus):\n",
    "    tokens = []\n",
    "    for i in (corpus):\n",
    "        tokens.append(i.split())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correspondence between natural numbers and words\n",
    "\n",
    "k = 0\n",
    "word2id = dict()\n",
    "for i in tokens:\n",
    "    for word in i:\n",
    "        if (word not in word2id):\n",
    "            word2id[word] = k\n",
    "            k += 1\n",
    "id2word = dict((v,k) for k,v in word2id.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates context-center word pairs in the given window\n",
    "\n",
    "window = 2\n",
    "context = []\n",
    "\n",
    "for i in tokens:\n",
    "    for j in range(len(i)):\n",
    "        center = i[j]\n",
    "        id_cent = word2id[center]\n",
    "        for k in range(max(j - window, 0), j):\n",
    "            cont = i[k]\n",
    "            id_cont = word2id[cont]\n",
    "            context.append([id_cent, id_cont])\n",
    "        for k in range(j + 1, min(j + window + 1, len(i))):\n",
    "            cont = i[k]\n",
    "            id_cont = word2id[cont]\n",
    "            context.append([id_cent, id_cont])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1]\n",
      " [ 0  2]\n",
      " [ 1  0]\n",
      " [ 1  2]\n",
      " [ 1  3]\n",
      " [ 2  0]\n",
      " [ 2  1]\n",
      " [ 2  3]\n",
      " [ 3  1]\n",
      " [ 3  2]\n",
      " [ 4  5]\n",
      " [ 4  6]\n",
      " [ 5  4]\n",
      " [ 5  6]\n",
      " [ 5  7]\n",
      " [ 6  4]\n",
      " [ 6  5]\n",
      " [ 6  7]\n",
      " [ 6  8]\n",
      " [ 7  5]\n",
      " [ 7  6]\n",
      " [ 7  8]\n",
      " [ 7  9]\n",
      " [ 8  6]\n",
      " [ 8  7]\n",
      " [ 8  9]\n",
      " [ 9  7]\n",
      " [ 9  8]\n",
      " [ 4  5]\n",
      " [ 4  6]\n",
      " [ 5  4]\n",
      " [ 5  6]\n",
      " [ 5 10]\n",
      " [ 6  4]\n",
      " [ 6  5]\n",
      " [ 6 10]\n",
      " [ 6 11]\n",
      " [10  5]\n",
      " [10  6]\n",
      " [10 11]\n",
      " [11  6]\n",
      " [11 10]\n",
      " [ 4 12]\n",
      " [ 4 13]\n",
      " [12  4]\n",
      " [12 13]\n",
      " [12 14]\n",
      " [13  4]\n",
      " [13 12]\n",
      " [13 14]\n",
      " [13  2]\n",
      " [14 12]\n",
      " [14 13]\n",
      " [14  2]\n",
      " [14 15]\n",
      " [ 2 13]\n",
      " [ 2 14]\n",
      " [ 2 15]\n",
      " [15 14]\n",
      " [15  2]\n",
      " [ 4 12]\n",
      " [ 4 16]\n",
      " [12  4]\n",
      " [12 16]\n",
      " [12 14]\n",
      " [16  4]\n",
      " [16 12]\n",
      " [16 14]\n",
      " [16  2]\n",
      " [14 12]\n",
      " [14 16]\n",
      " [14  2]\n",
      " [14 17]\n",
      " [ 2 16]\n",
      " [ 2 14]\n",
      " [ 2 17]\n",
      " [17 14]\n",
      " [17  2]\n",
      " [ 4 18]\n",
      " [ 4 19]\n",
      " [18  4]\n",
      " [18 19]\n",
      " [18 20]\n",
      " [19  4]\n",
      " [19 18]\n",
      " [19 20]\n",
      " [19 21]\n",
      " [20 18]\n",
      " [20 19]\n",
      " [20 21]\n",
      " [20 22]\n",
      " [21 19]\n",
      " [21 20]\n",
      " [21 22]\n",
      " [21  4]\n",
      " [22 20]\n",
      " [22 21]\n",
      " [22  4]\n",
      " [22 23]\n",
      " [ 4 21]\n",
      " [ 4 22]\n",
      " [ 4 23]\n",
      " [23 22]\n",
      " [23  4]\n",
      " [ 4  5]\n",
      " [ 4 24]\n",
      " [ 5  4]\n",
      " [ 5 24]\n",
      " [ 5  2]\n",
      " [24  4]\n",
      " [24  5]\n",
      " [24  2]\n",
      " [24 25]\n",
      " [ 2  5]\n",
      " [ 2 24]\n",
      " [ 2 25]\n",
      " [25 24]\n",
      " [25  2]\n",
      " [10  4]\n",
      " [10 26]\n",
      " [ 4 10]\n",
      " [ 4 26]\n",
      " [ 4 27]\n",
      " [26 10]\n",
      " [26  4]\n",
      " [26 27]\n",
      " [26  4]\n",
      " [27  4]\n",
      " [27 26]\n",
      " [27  4]\n",
      " [27 26]\n",
      " [ 4 26]\n",
      " [ 4 27]\n",
      " [ 4 26]\n",
      " [26 27]\n",
      " [26  4]\n",
      " [10  4]\n",
      " [10  5]\n",
      " [ 4 10]\n",
      " [ 4  5]\n",
      " [ 4 28]\n",
      " [ 5 10]\n",
      " [ 5  4]\n",
      " [ 5 28]\n",
      " [ 5 29]\n",
      " [28  4]\n",
      " [28  5]\n",
      " [28 29]\n",
      " [28 30]\n",
      " [29  5]\n",
      " [29 28]\n",
      " [29 30]\n",
      " [29 31]\n",
      " [30 28]\n",
      " [30 29]\n",
      " [30 31]\n",
      " [30 32]\n",
      " [31 29]\n",
      " [31 30]\n",
      " [31 32]\n",
      " [31 33]\n",
      " [32 30]\n",
      " [32 31]\n",
      " [32 33]\n",
      " [32  7]\n",
      " [33 31]\n",
      " [33 32]\n",
      " [33  7]\n",
      " [33 34]\n",
      " [ 7 32]\n",
      " [ 7 33]\n",
      " [ 7 34]\n",
      " [34 33]\n",
      " [34  7]]\n"
     ]
    }
   ],
   "source": [
    "context = np.array(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "for i in data:\n",
    "    cont = []\n",
    "    for t in context:\n",
    "        if (t[0] == i):\n",
    "            cont.append(t[1])\n",
    "    target.append(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dset\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, Sampler\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot word representation\n",
    "def get_input(word_id):\n",
    "    x = np.zeros(vocab_size)\n",
    "    x[word_id] = 1.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr =  0.001 , epoch =  101\n",
      "lr =  0.01 , epoch =  101\n"
     ]
    }
   ],
   "source": [
    "#finding the best lr and epoch number to get the minimal loss\n",
    "\n",
    "min_loss = 100\n",
    "for num_epochs in range(101, 120):\n",
    "    lr = 0.001\n",
    "    while (lr <= 0.1):\n",
    "        w1 = Variable(torch.randn(emb_dim, vocab_size).float(), requires_grad = True)\n",
    "        w2 = Variable(torch.randn(vocab_size, emb_dim).float(), requires_grad = True)\n",
    "        glob_loss = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            loss_val = 0\n",
    "            for d, target in context:\n",
    "                x = Variable(torch.from_numpy(get_input(d))).float()\n",
    "                targ = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "                z1 = torch.matmul(w1, x)\n",
    "                z2 = torch.matmul(w2, z1)\n",
    "                log_sfm = F.log_softmax(z2, dim = 0)\n",
    "                \n",
    "                loss = F.nll_loss(log_sfm.view(1, -1), targ)\n",
    "                loss_val += loss.data.item()\n",
    "                loss.backward()\n",
    "                w1.data -= lr * w1.grad.data\n",
    "                w2.data -= lr * w2.grad.data\n",
    "                w1.grad.data.zero_()\n",
    "                w2.grad.data.zero_()\n",
    "\n",
    "            glob_loss = loss_val / len(context)\n",
    "        if (glob_loss < min_loss):\n",
    "            min_loss = glob_loss\n",
    "            print(\"lr = \", lr, \", epoch = \", num_epochs)\n",
    "        lr *= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  4.958078961262758\n",
      "loss =  3.5261169891247803\n",
      "loss =  3.1415076968313635\n",
      "loss =  2.9157948137699874\n",
      "loss =  2.740347830728553\n",
      "loss =  2.5890819334435737\n",
      "loss =  2.4573561268291253\n",
      "loss =  2.3464382763566642\n",
      "loss =  2.2564077089572776\n",
      "loss =  2.1845423421640504\n",
      "loss =  2.1278072253040885\n"
     ]
    }
   ],
   "source": [
    "#to see performance for the best lr-epoch pair\n",
    "\n",
    "num_epochs = 101\n",
    "lr = 0.01\n",
    "w1 = Variable(torch.randn(emb_dim, vocab_size).float(), requires_grad = True)\n",
    "w2 = Variable(torch.randn(vocab_size, emb_dim).float(), requires_grad = True)\n",
    "for epoch in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for d, target in context:\n",
    "        x = Variable(torch.from_numpy(get_input(d))).float()\n",
    "        targ = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "        z1 = torch.matmul(w1, x)\n",
    "        z2 = torch.matmul(w2, z1)\n",
    "        log_sfm = F.log_softmax(z2, dim = 0)\n",
    "\n",
    "        loss = F.nll_loss(log_sfm.view(1, -1), targ)\n",
    "        loss_val += loss.data.item()\n",
    "        loss.backward()\n",
    "        w1.data -= lr * w1.grad.data\n",
    "        w2.data -= lr * w2.grad.data\n",
    "        w1.grad.data.zero_()\n",
    "        w2.grad.data.zero_()\n",
    "    if (epoch % 10 == 0):\n",
    "        print(\"loss = \", loss_val/len(context))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
